@article{WU2023110331,
title = {Advanced acoustic footstep-based person identification dataset and method using multimodal feature fusion},
journal = {Knowledge-Based Systems},
volume = {264},
pages = {110331},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110331},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123000813},
author = {Shichao Wu and Xiaolin Zhai and Zhengxi Hu and Yue Sun and Jingtai Liu},
keywords = {Acoustic footstep, Person identification, Deep neural network, Microphone array, Audio dataset},
abstract = {Automatic person identification based on the acoustic footstep collected from the microphone array has fewer privacy considerations compared to the camera-dependent solutions. Moreover, the time difference of sound arrival among multiple audio channels could enable the person direction estimation, which is helpful for robots to perform the person following task in a socially compliant manner. As far as we know, the acoustic footstep benchmark dataset built for robots from the microphone array has remained unexplored. In this paper, we propose to build one improved acoustic footstep-based person identification dataset (AFPID-II). It is designed to involve various experimental covariates that are commonly considered to degrade the footstep recognition accuracy. Specifically, we build the AFPID-II dataset from 41 subjects with a lightweight microphone array in unstrained indoor rooms. We consider the covariates of clothes, shoes, and different rooms for the acoustic footstep collection. The AFPID-II dataset contains over 14Â h of footstep audios (around 88,467 footstep events). It is much more plentiful compared to the former SFootBD and TUM GAID datasets. We also present one baseline identification method (AFPI-Net), which fully excavates the acoustic features in one multimodal feature fusion manner. Experimental results showed that room type degrades the identification severely (around 78%), followed by shoe type (31%) and clothing (13%).}
}