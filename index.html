<!DOCTYPE HTML>


<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shichao Wu</title>
  
  <meta name="author" content="Shichao Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/robots-and-humans.png">
  
</head> 


<body>
<table style="width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody><tr>
        <td halign="center">
            <p align="center">
                <font size="6">Shichao Wu   吴仕超</font>
            </p>
        </td>
    </tr>
    <tr>
        <td>
            <table width="100%" border="0" align="center" cellspacing="20">
                <tbody><tr>
                    <!--<td width="70%" valign="middle"  align="justify">-->
					<td width="70%" valign="middle">
                      <p>Shichao Wu, Associate Researcher at Southwest University of Science and Technology 
						</p>
					  <p>Shichao Wu is an Associate Researcher at Southwest University of Science and Technology and 
					  a Researcher at the Robot Technology Used for Special Environment Key Laboratory of Sichuan Province, 
					  School of Information and Control Engineering. 					  
					  His primary research areas include acoustic computing and spatial perception intelligence. 
					  He has published a total of 30 academic papers, including 18 SCI papers, 13 top journal papers, and 1 highly cited paper. 
					  Previously, he obtained the B.E. from Southwest University of Science and Technology in 2017, the M.S. from Northeastern University in 2020, 
					  and the Ph.D. in Artificial Intelligence from Nankai University in 2024, under the supervision of Prof. Jingtai Liu.
						</p>
						
					  <p>吴仕超，副研究员（硕士研究生导师），信息与控制工程学院，特殊环境机器人技术四川省重点实验室研究人员。
					  主要研究方向为声学计算、空间感知智能，已发表学术论文共30篇，其中SCI期刊论文18篇，TOP期刊论文13篇，高被引论文1篇。
					  在此之前，他于2017年在西南科技大学取得学士学位，2020年在东北大学取得硕士学位，
					  2024年在南开大学取得博士学位（人工智能专业，导师：刘景泰教授）。
						</p>
					  <p style="text-align:center">
						<a href="shichaowu199@gmail.com">Email</a> &nbsp/&nbsp
						<!--<a href="data/context_er_intro.pdf">CV</a> &nbsp/&nbsp-->
						<a href="https://scholar.google.com/citations?hl=zh-CN&user=dIBB8vsAAAAJ">Google Scholar</a> &nbsp/&nbsp
						<a href="https://blog.csdn.net/cratial">CSDN</a> &nbsp/&nbsp
						<a href="https://github.com/Cratial">Github</a>
					  </p>						
                    </td>
                    <td width="100%" valign="middle" align="center">
                        <img src="images/wusc.png" width="87%">
                    </td>
                </tr>
            </tbody></table>
			
			
			
		<!-- News Part -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading><font color="red">News！！！</font></heading>
					<font color="red">					
					<p><strong>课题组现招 2026 年 9 月入学的全日制硕士研究生 2 名。</strong></p></font>
					
					<font color="blue">
					<p>专业可报控制科学与工程（机器人科学与工程、模式识别与智能系统）、新一代电子信息技术、控制工程。</p>
					<p>研究方向为视听融合感知、具身智能机器人导航与操作。</p>					
					  
					<p>现诚邀有意向者发送邮件至 [wusc@swust.edu.cn、cratial@163.com]，邮件主题请注明「2026 硕士报考 - 姓名 - 本科专业」，附件需包含可证明你科研兴趣与当前状态的相关资料，我会在收到邮件后 1-3 个工作日内回复，期待与对科研有热情、有潜力的你共同成长！
					</p>
					</font>
				</td>
			  </tr>
			</tbody>
			</table>
			
		
		<!-- Project Part -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Project</heading>
				  <p>
					As the Principal Investigator (PI), I have led multiple research projects, including one 
					Natural Science Foundation of Sichuan Province, one National Natural Science Foundation of China (NSFC)
					Youth Fund, and one Outstanding Talent Cultivation Fund Project at Southwest University of Science and Technology.

					In addition, I have participated one Natural Science Foundation of Sichuan Province
					(General Program) and one NSFC General Program.
				  </p>
				</td>
			  </tr>
			</tbody>
			</table>
			
			
		<!-- Research Part -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Research</heading>
				  <p>					
					I’m currently engaged in and plan to research spatial perception intelligence for robotics. 
					These works primarily focus on (a) acoustic computing; 
					(b) audio-visual perception; 
					and (c) spatial perception intelligence-enabled dexterous manipulation and embodied navigation. 
					These works tried to perceive and understand the spatial characteristics of humans and objects
					from both visual and auditory perspectives, thereby enabling Generalist Robots to 
					operate (navigate and manipulate) effectively in various complex scenarios.
				  </p>
				</td>
			  </tr>
			</tbody></table>
			
		<!-- Selected Publication Part -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Selected Publications</heading>
				  <p>
				    My publications can be found <a href="publication/index.html">here</a>. See also: <a href="https://scholar.google.com/citations?hl=zh-CN&user=dIBB8vsAAAAJ">Google Scholar</a>.
					Here are some recent publications.
					Representative papers are <span class="highlight">highlighted</span>.
				  </p>
				</td>
			  </tr>
			</tbody></table>
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/dfscda.png"><img src="images/dfscda.png" alt="dfscda" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://doi.org/10.1016/j.eswa.2025.128827", target="_blank" id="dfscda">
					<papertitle>DFSC-DA: Dominant Frequency Segmented Conformer with Domain Adaptation for Acoustic Footstep-based Person Identification</papertitle>
				  </a>
				  <br>
				  <strong>Shichao Wu*</strong>, Jinzheng Guang, Wei Wu, Gongping Chen.
				  <br>
				  <em>Expert Systems With Applications</em>, 2025
				  <br>
				  <a href="https://doi.org/10.1016/j.eswa.2025.128827", target="_blank">paper</a> | <a href="data/dfscda.bib", target="_blank">bibtex</a> 
				  <p>We design a Dominant Frequency Segmented Conformer with Domain Adaptation (DFSC-DA) to enhance acoustic footstep-based person identification under interferences (clothing, shoe type, room type). It segments raw footsteps by dominant frequencies, models intra/inter-period variations via Conformer, and uses adversarial training for robust features. </p>
				</td>
			  </tr>
			
			
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/crati.png"><img src="images/crati.png" alt="crati" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://doi.org/10.1016/j.knosys.2024.112692", target="_blank" id="crati">
					<papertitle>CRATI: Contrastive Representation-based Multimodal Sound Event Localization and Detection</papertitle>
				  </a>
				  <br>
				  <strong>Shichao Wu</strong>, Yongru Wang, Yushan Jiang, Qianyi Zhang, Jingtai Liu*.
				  <br>
				  <em>Knowledge-Based Systems</em>, 2024
				  <br>
				  <a href="https://doi.org/10.1016/j.knosys.2024.112692", target="_blank">paper</a> | <a href="data/crati.bib", target="_blank">bibtex</a>  
				  <p>We propose a contrastive representation-based multimodal acoustic model (CRATI) for sound event localization and detection (SELD), which learns robust audio representations from audio, text, and image in an end-to-end manner.</p>
				</td>
			  </tr>
			  
			  
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr  bgcolor="#ffffd0">
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/afpild.png"><img src="images/afpild.png" alt="afpild" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://doi.org/10.1016/j.inffus.2023.102181", target="_blank" id="afpild">
					<papertitle>AFPILD: Acoustic Footstep Dataset Collected Using One Microphone Array and LiDAR Sensor for Person Identification and Localization</papertitle>
				  </a>
				  <br>
				  <strong>Shichao Wu</strong>, Shouwang Huang, Zicheng Liu, Qianyi Zhang, Jingtai Liu*.
				  <br>
				  <em>Information Fusion</em>, 2024
				  <br>
				  <a href="https://doi.org/10.1016/j.inffus.2023.102181", target="_blank">paper</a> | <a href="data/afpild.bib", target="_blank">bibtex</a> | <a href="https://github.com/NkuSRLab/AFPILD-CRNN", target="_blank"><font color="red">dataset & code</font></a> 
				  <p>We build the acoustic footstep-based person identification and localization dataset (AFPILD) by unifying the identify and locate tasks for the first time, concerning the clothing and shoe type covariates.</p>
				</td>
			  </tr>
			  
			  
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/haac.png"><img src="images/haac.png" alt="haac" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://doi.org/10.1016/j.apacoust.2023.109541", target="_blank" id="haac">
					<papertitle>HAAC: Hierarchical Audio Augmentation Chain for ACCDOA Described Sound Event Localization and Detection</papertitle>
				  </a>
				  <br>
				  <strong>Shichao Wu</strong>, Yongru Wang, Zhengxi Hu, Jingtai Liu*.
				  <br>
				  <em>Applied Acoustics</em>, 2023
				  <br>
				  <a href="https://doi.org/10.1016/j.apacoust.2023.109541", target="_blank">paper</a> | <a href="data/haac.bib", target="_blank">bibtex</a> | <a href="https://github.com/NkuSRLab/HAAC-SELD", target="_blank">code</a> 
				  <p>We propose one hierarchical audio augmentation chain (HAAC) that contains feature map augmentation, audio channel swapping, and sample mixup to augment the audio features for the SELD task.  </p>
				</td>
			  </tr>			
			
						
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr  bgcolor="#ffffd0">
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/afpid.png"><img src="images/afpid.png" alt="afpid" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://www.sciencedirect.com/science/article/pii/S0950705123000813", target="_blank" id="afpid">
					<papertitle>Advanced Acoustic Footstep-based Person Identification Dataset and Method Using Multimodal Feature Fusion</papertitle>
				  </a>
				  <br>
				  <strong>Shichao Wu</strong>, Xiaolin Zhai, Zhengxi Hu, Yue Sun, Jingtai Liu*.
				  <br>
				  <em>Knowledge-Based Systems (KBS)</em>, 2023
				  <br>
				  <a href="https://www.sciencedirect.com/science/article/pii/S0950705123000813", target="_blank">paper</a> | <a href="data/afpid.bib", target="_blank">bibtex</a> | <a href="https://github.com/NkuSRLab/AFPIDII-AFPINet", target="_blank"><font color="red">dataset & code</font></a> 
				  <p>We propose one improved acoustic footstep-based person identification dataset (AFPID-II) from 41 subjects, counting over 14 h of footstep audios.</p>
				</td>
			  </tr>
			

			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr  bgcolor="#ffffd0">
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/context_er_intro.png"><img src="images/context_er_intro.png" alt="context_er" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/9868807", target="_blank" id="context_er">
					<papertitle>Hierarchical Context-Based Emotion Recognition with Scene Graphs</papertitle>
				  </a>
				  <br>
				  <strong>Shichao Wu</strong>, Lei Zhou, Zhengxi Hu, Jingtai Liu*.
				  <br>
				  <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</em>, 2022
				  <br>
				  <a href="https://ieeexplore.ieee.org/document/9868807", target="_blank">paper</a> | <a href="data/context_er.bib", target="_blank">bibtex</a>
				  <p>We propose the hierarchical contexts (the entity context, the global context, and the scene context) based emotion recognition method with scene graphs. </p>
				</td>
			  </tr>
			  
			  
			</tbody></table>
			
				
		<!-- Previous Research Part -->
		<!--	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Previous publications (works before 2020)</heading>
				  <p>
					Before I start my journey at NKU, I studied at SWUST with my partners concerning robotics under the adviser of <a href="http://www.info.swust.edu.cn/index.php?a=index&f=tc&aid=208&id=21", target="_blank">A.P. Manlu Liu</a>
					and <a href="http://www.info.swust.edu.cn/index.php?a=index&f=tc&aid=89&id=21", target="_blank">Prof. Hua Zhang</a> from 06/2014 to 06/2017 in Robot Technology Used for Special Environment Key Laboratory of Sichuan Province.
					</p>
					<p>
					After that, I spent two and a half years working with <a href="http://faculty.neu.edu.cn/neu_wangfei/zh_CN/yjgk/46178/list/index.htm", target="_blank">Prof. Fei Wang</a>
					on EEG-based fatigue detection & emotion recognition, and brain-computer interface from 09/2017 to 01/2020 in the human-robot cooperation laboratory.
					Representative papers are <span class="highlight">highlighted</span>.
				  </p>
				</td>
			  </tr>
			</tbody></table>
			-->	
					

			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/fatigue_bfn_img.png"><img src="images/fatigue_bfn_img.png" alt="fatigue_bfn" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://ieeexplore.ieee.org/document/9358987", target="_blank" id="fatigue_bfn">
					<papertitle>EEG Driving Fatigue Detection with PDC-Based Brain Functional Network</papertitle>
				  </a>
				  <br>
				  Fei Wang*, <strong>Shichao Wu</strong>, Jingyu Ping, Zongfeng Xu, Hao Chu.
				  <br>
				  <em>IEEE Sensors Journal</em>, 2021
				  <br>
				  <a href="https://ieeexplore.ieee.org/document/9358987", target="_blank">paper</a> | <a href="data/fatigue_bfn.bib", target="_blank">bibtex</a>
				  <p>Propose one driving fatigue detection approach based on the brain functional network, and provide one way for the key EEG electrodes and rhythms selection.</p>
				</td>
			  </tr>
			</tbody></table>
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr bgcolor="#ffffd0">
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/emo_efdms_img.jpg"><img src="images/emo_efdms_img.jpg" alt="emo_efdms" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://www.sciencedirect.com/science/article/pii/S0028393220301780", target="_blank" id="emo_efdms">
					<papertitle>Emotion Recognition with Convolutional Neural Network and EEG-based EFDMs</papertitle>
				  </a>
				  <br>
				  Fei Wang*, <strong>Shichao Wu</strong>, Weiwei Zhang, Zongfeng Xu, Yahui Zhang, Chengdong Wu, Sonya Coleman.
				  <br>
				  <em>Neuropsychologia</em>, 2020
				  <br>
				  <a href="https://www.sciencedirect.com/science/article/pii/S0028393220301780", target="_blank">paper</a> | <a href="data/emo_efdms.bib", target="_blank">bibtex</a>
				  <p>Propose the EFDMs based on EEG with STFT, and combine them with the deep convolutional neural network for emotion recognition.
				  </p>
				</td>
			  </tr>
			</tbody></table>
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/fatigue_nonlinear_fea_img.png"><img src="images/fatigue_nonlinear_fea_img.png" alt="fatigue_nonfea" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://www.sciencedirect.com/science/article/pii/S1746809420302317", target="_blank" id="fatigue_nonfea">
					<papertitle>Multiple Nonlinear Features Fusion Based Driving Fatigue Detection</papertitle>
				  </a>
				  <br>
				  Fei Wang*, <strong>Shichao Wu</strong>, Weiwei Zhang, Zongfeng Xu, Yahui Zhang, Hao Chu.
				  <br>
				  <em>Biomedical Signal Processing and Control</em>, 2020
				  <br>
				  <a href="https://www.sciencedirect.com/science/article/pii/S1746809420302317", target="_blank">paper</a> | <a href="data/fatigue_nonlinear_fea.bib", target="_blank">bibtex</a>
				  <p>Propose one driving fatigue detection method based on multiple nonlinear features fused with multiple kernel learning (MKL) based SVM.
				  </p>
				</td>
			  </tr>
			</tbody></table>
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				  <a href="images/fatigue_transfer_img.png"><img src="images/fatigue_transfer_img.png" alt="fatigue_transfer" width="240" height="160" style="border-style: none"></a>
				</td>
				<td width="75%" valign="middle">
				  <a href="https://jeit.ac.cn/cn/article/doi/10.11999/JEIT180900", target="_blank" id="fatigue_transfer">
					<papertitle>Driver Fatigue Detection Through Deep Transfer Learning in an Electroencephalogram-based System</papertitle>
				  </a>
				  <br>
				  Fei Wang*, <strong>Shichao Wu</strong>, Shaolin Liu, Yahui Zhang, Ying Wei.
				  <br>
				  <em>Journal of Electronics & Information Technology</em>, [in Chinese], 2019
				  <br>				  
				  <a href="https://jeit.ac.cn/cn/article/doi/10.11999/JEIT180900", target="_blank">paper</a> | <a href="data/fatigue_transfer.txt", target="_blank">bibtex</a>
				  <p>Propose one fatigue detection approch with transfer learning based on the Electrode-Frequency Distribution Maps (EFDMs) of EEG signals.
				  </p>
				  <p>
				  <strong><font color="red">"Highly Cited Paper of 2019"</font></strong> in the <a href="http://www.myzaker.com/article/61f3a5348e9f092764088684/", target="_blank" id="fatigue_transfer">Journal of Electronics & Information Technology</a>
				  </p>
				</td>
			  </tr>
			</tbody></table>
			
			
			
			
		<!-- Academic Service -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Academic Service</heading>
				</td>
			  </tr>
			</tbody></table>
			
			
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  <tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				<img src="images/kobby-mendez-d0oYF8hm4GI-unsplash.jpg" alt="reviewer_img" width="240" style="border-style: none"></td>
				<td width="75%" valign="center">
				  
				  <p><strong>Early-Career Editorial Board</strong> of <em> Robot Learning</em></p>
				  <p><strong>Reviewer</strong> of IEEE Transactions on Neural Networks and Learning Systems (TNNLS),
				  Engineering Applications of Artificial Intelligence (EAAI), 
				  International Journal of Intelligent Systems, 
				  Applied Intelligence, IROS, et al.
				  </p>
				</td>
			  </tr>
			  
			</tbody></table>
			
		<!-- Thanks for this website template -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  <tr>
				<td style="padding:0px">
				  <br>
				  <p style="text-align:right;font-size:small;">
					<br>
					Special thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>
				  </p>
				  <p style="text-align:right;font-size:small;">
					<br>
					Last updated: Oct. 2025
				  </p>
				  
				</td>
			  </tr>
			</tbody></table>
			
		<!-- Website Acess Statistics -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
				  <heading>Visitor statistics (monthly)</heading>
				</td>
			  </tr>
			</tbody></table>
		<!-- Acess location maps -->
			<table style="width:50%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
			  <td style="padding:25px;width:25%;vertical-align:middle">
				<p style="text-align:center;font-size:small;">
				  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Nf9JLSPnh5kV19cJJX3LJrqsxQ0KiJ7YmpagTwstFxE&cl=ffffff&w=a"></script>
				  <!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=Nf9JLSPnh5kV19cJJX3LJrqsxQ0KiJ7YmpagTwstFxE&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353"></script>  Total Pageviews -->
				</p>
			  </td>
			</tr>
			</tbody></table>

        </td>
    </tr>
</tbody></table>


</body></html>